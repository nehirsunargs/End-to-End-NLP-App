# -*- coding: utf-8 -*-
"""CIS4190.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1a1Br8WhSZlR66cbgFE9eS3SZksMAyuHb
"""

import pandas as pd
import requests
from bs4 import BeautifulSoup
import time

url_df = pd.read_csv('/content/url_only_data.csv', header=None, names=['url'])

url_df['source'] = ['FoxNews' if i < 2010 else 'NBC' for i in range(len(url_df))]

def scrape_headline(url, source):
    try:
        response = requests.get(url, timeout=5)
        if response.status_code != 200:
            return None
        soup = BeautifulSoup(response.text, 'html.parser')

        if source == 'FoxNews':
            h1 = soup.find('h1', class_='headline speakable')
        else:
            h1 = soup.find('h1')

        return h1.text.strip() if h1 else None
    except Exception as e:
        return None

headlines = []
for idx, row in url_df.iterrows():
    headline = scrape_headline(row['url'], row['source'])
    headlines.append(headline)
    time.sleep(0.5)

url_df['headline'] = headlines

url_df = url_df.dropna(subset=['headline'])
url_df.to_csv('/content/scraped_headlines.csv', index=False)

url_df.head()

import re
import string

df = pd.read_csv('/content/scraped_headlines.csv')

# Clean headlines
def clean_text(text):
    text = text.lower()
    text = re.sub(r'<.*?>', '', text)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df['cleaned_headline'] = df['headline'].apply(clean_text)

df['label'] = df['source'].apply(lambda x: 1 if x == 'FoxNews' else 0)

df.to_csv('/content/cleaned_headlines.csv', index=False)

df[['cleaned_headline', 'label']].head()

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score

df = pd.read_csv('/content/cleaned_headlines.csv')

X = df['cleaned_headline']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(stop_words='english', max_features=100)
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

model = LogisticRegression(max_iter=100)
model.fit(X_train_tfidf, y_train)

y_pred = model.predict(X_test_tfidf)

accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
print("Classification Report:\n", classification_report(y_test, y_pred))

from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
import pandas as pd

df = pd.read_csv('/content/cleaned_headlines.csv')
X = df['cleaned_headline']
y = df['label']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

vectorizer = TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1,2))
X_train_tfidf = vectorizer.fit_transform(X_train)
X_test_tfidf = vectorizer.transform(X_test)

models = {
    "Logistic Regression": LogisticRegression(max_iter=200),
    "Naive Bayes": MultinomialNB(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42)
}

for name, model in models.items():
    model.fit(X_train_tfidf, y_train)
    y_pred = model.predict(X_test_tfidf)
    acc = accuracy_score(y_test, y_pred)
    print(f"\n{name} Accuracy: {acc:.4f}")
    print(classification_report(y_test, y_pred))

from sklearn.naive_bayes import MultinomialNB

df = pd.read_csv('/content/cleaned_headlines.csv')
X = df['cleaned_headline']
y = df['label']

vectorizer = TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1,2))
X_tfidf = vectorizer.fit_transform(X)

model = MultinomialNB()
model.fit(X_tfidf, y)

def predict_news_source(headlines):
    cleaned = [re.sub(r'[^a-zA-Z0-9\s]', '', h.lower()).strip() for h in headlines]
    tfidf = vectorizer.transform(cleaned)
    preds = model.predict(tfidf)
    labels = ['NBC' if p == 0 else 'FoxNews' for p in preds]
    return labels

sub_testset = [
    "Biden outlines new education policy during speech in Pennsylvania",
    "Fox News anchor clashes with guest over inflation numbers",
    "NBC exclusive: Inside the Pentagon's new space command",
    "Trump holds rally amid criminal investigations and indictments",
    "NBC reports on increasing rent prices across the country",
    "Fox News investigates Hunter Biden business dealings",
    "Supreme Court to hear major case on abortion access",
    "Fox News personality criticizes media coverage of border crisis",
    "NBC hosts town hall on climate change initiatives",
    "Fox & Friends segment covers California's crime rates",
    "NBC reveals findings from latest COVID-19 study",
    "Fox News panel debates Biden's foreign policy stance",
    "NBC analyst discusses 2024 election predictions",
    "Fox News questions timing of DOJ investigation",
    "NBC coverage of women's rights march in Washington",
    "Fox News airs interview with conservative think tank leader",
    "NBC reports on new infrastructure bill progress",
    "Fox News criticizes 'woke' culture in universities",
    "NBC poll shows declining trust in government",
    "Fox News anchor challenges guest on tax policy"
]

predictions = predict_news_source(sub_testset)
for i, (headline, pred) in enumerate(zip(sub_testset, predictions)):
    print(f"{i+1}. [{pred}] {headline}")

import joblib

joblib.dump({"model": model, "vectorizer": vectorizer}, "naive_bayes_model.pkl")

from huggingface_hub import login

login()

from huggingface_hub import upload_file

upload_file(
    path_or_fileobj="naive_bayes_model.pkl",
    path_in_repo="naive_bayes_model.pkl",
    repo_id="nehirsunargs/nb-model",
    repo_type="model"
)

import pandas as pd
import re
import joblib
from huggingface_hub import hf_hub_download

df = pd.read_csv('final_test_data.csv')

# Load model and vectorizer from Hugging Face
model_bundle = joblib.load(
    hf_hub_download("nehirsunargs/nb-model", "naive_bayes_model.pkl")
)
model = model_bundle["model"]
vectorizer = model_bundle["vectorizer"]

# Clean headlines
def clean_text(text):
    return re.sub(r'[^a-zA-Z0-9\s]', '', str(text).lower()).strip()

df['cleaned'] = df['Headline'].apply(clean_text)
X_test = vectorizer.transform(df['cleaned'])
preds = model.predict(X_test)

# Convert numerical predictions to label strings
df['Label(FoxNews/NBC)'] = ['FoxNews' if p == 1 else 'NBC' for p in preds]

submission_df = df[['ID', 'Headline', 'Label(FoxNews/NBC)']]

submission_df.to_csv('test_submission.csv', index=False)

files.download('test_submission.csv')